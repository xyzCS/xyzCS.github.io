---
title: "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers"
collection: publications
category: conferences
permalink: /publication/scireplicate-bench
excerpt: 'Benchmarking agentic LLM systems on reproducing algorithms described in research papers.'
date: 2025-04-06
venue: 'COLM 2025 (under review)'
paperurl: 'https://arxiv.org/abs/2504.00255'
citation: 'Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, Yulan He. 2025. "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers." Under review at <i>COLM 2025</i>.'
authors: '<strong>Yanzheng Xiang</strong>, Hanqi Yan, Shuyin Ouyang, Lin Gui, Yulan He'
paper_label: 'arXiv'
---
We introduce SciReplicate-Bench, the first benchmark targeting the reliable reproduction of algorithmic results described in scientific papers with agentic LLM pipelines. The benchmark covers memory management, tool grounding, and execution tracking, enabling systematic evaluation of agent behaviors. Project resources are available on the [website](https://xyzcs.github.io/scireplicate.github.io/).
